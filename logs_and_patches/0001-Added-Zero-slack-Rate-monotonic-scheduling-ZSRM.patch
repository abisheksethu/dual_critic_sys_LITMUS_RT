From 3a03f0699b304a5d01cc97df5e54aeab62cca4cc Mon Sep 17 00:00:00 2001
From: Sethupandi Abishek <abishek001@e.ntu.edu.sg>
Date: Fri, 5 May 2017 13:20:46 +0800
Subject: [PATCH] Added Zero slack Rate monotonic scheduling - ZSRM

Note: Other plugins does not work because of launching zero slack timer in budget.c
---
 include/litmus/fp_common.h       |   2 +
 include/litmus/litmus.h          |   7 +
 include/litmus/rt_param.h        |  14 +-
 include/litmus/sched_plugin.h    |  10 +-
 include/litmus/zeroslack_timer.h |   8 +
 litmus/Makefile                  |   4 +-
 litmus/budget.c                  |   2 +
 litmus/sched_zsrm.c              | 664 +++++++++++++++++++++++++++++++++++++++
 litmus/zeroslack_timer.c         | 110 +++++++
 9 files changed, 818 insertions(+), 3 deletions(-)
 create mode 100644 include/litmus/zeroslack_timer.h
 create mode 100644 litmus/sched_zsrm.c
 create mode 100644 litmus/zeroslack_timer.c

diff --git a/include/litmus/fp_common.h b/include/litmus/fp_common.h
index 19356c0..24d6501 100644
--- a/include/litmus/fp_common.h
+++ b/include/litmus/fp_common.h
@@ -17,6 +17,8 @@ int fp_higher_prio(struct task_struct* first,
 
 int fp_ready_order(struct bheap_node* a, struct bheap_node* b);
 
+int rm_ready_order(struct bheap_node* a, struct bheap_node* b);
+
 #define FP_PRIO_BIT_WORDS (LITMUS_MAX_PRIORITY / BITS_PER_LONG)
 
 #if (LITMUS_MAX_PRIORITY % BITS_PER_LONG)
diff --git a/include/litmus/litmus.h b/include/litmus/litmus.h
index 0519831..c9f97a8 100644
--- a/include/litmus/litmus.h
+++ b/include/litmus/litmus.h
@@ -64,11 +64,14 @@ void litmus_do_exit(struct task_struct *tsk);
 
 /* task_params macros */
 #define get_exec_cost(t)  	(tsk_rt(t)->task_params.exec_cost)
+#define get_exec_cost_hi(t)  	(tsk_rt(t)->task_params.exec_cost_hi)
 #define get_rt_period(t)	(tsk_rt(t)->task_params.period)
 #define get_rt_relative_deadline(t)	(tsk_rt(t)->task_params.relative_deadline)
+#define get_rt_relative_zero_slack(t)	(tsk_rt(t)->task_params.relative_zero_slack)
 #define get_rt_phase(t)		(tsk_rt(t)->task_params.phase)
 #define get_partition(t) 	(tsk_rt(t)->task_params.cpu)
 #define get_priority(t) 	(tsk_rt(t)->task_params.priority)
+#define get_criticality(t)	(tsk_rt(t)->task_params.crit)
 #define get_class(t)        (tsk_rt(t)->task_params.cls)
 #define get_release_policy(t) (tsk_rt(t)->task_params.release_policy)
 
@@ -77,6 +80,7 @@ void litmus_do_exit(struct task_struct *tsk);
 #define get_deadline(t)		(tsk_rt(t)->job_params.deadline)
 #define get_release(t)		(tsk_rt(t)->job_params.release)
 #define get_lateness(t)		(tsk_rt(t)->job_params.lateness)
+#define get_job_completed(t)(tsk_rt(t)->job_params.job_completed)
 
 /* release policy macros */
 #define is_periodic(t)		(get_release_policy(t) == TASK_PERIODIC)
@@ -112,6 +116,9 @@ static inline lt_t litmus_clock(void)
 	((t)->state == TASK_RUNNING || 	\
 	 task_thread_info(t)->preempt_count & PREEMPT_ACTIVE)
 
+#define is_job_running(t)		\
+	(get_job_completed(t) == TASK_RUNNING)
+
 #define is_blocked(t)       \
 	(!is_running(t))
 #define is_released(t, now)	\
diff --git a/include/litmus/rt_param.h b/include/litmus/rt_param.h
index e26535b..f6931b2 100644
--- a/include/litmus/rt_param.h
+++ b/include/litmus/rt_param.h
@@ -71,16 +71,24 @@ typedef enum {
 	((p) >= LITMUS_HIGHEST_PRIORITY &&	\
 	 (p) <= LITMUS_LOWEST_PRIORITY)
 
+typedef enum {
+	LOW,
+	HIGH
+} Criticallity;
+
 struct rt_task {
 	lt_t 		exec_cost;
+	lt_t 		exec_cost_hi;
 	lt_t 		period;
 	lt_t		relative_deadline;
+	lt_t 		relative_zero_slack;
 	lt_t		phase;
 	unsigned int	cpu;
 	unsigned int	priority;
 	task_class_t	cls;
 	budget_policy_t  budget_policy;  /* ignored by pfair */
 	release_policy_t release_policy;
+	Criticallity	crit;
 };
 
 union np_flag {
@@ -120,7 +128,7 @@ struct control_page {
 	uint64_t ts_syscall_start;  /* Feather-Trace cycles */
 	uint64_t irq_syscall_start; /* Snapshot of irq_count when the syscall
 				     * started. */
-
+	uint32_t active_crit; /*Active system criticality.*/
 	/* to be extended */
 };
 
@@ -162,6 +170,9 @@ struct rt_job {
 	 * Increase this sequence number when a job is released.
 	 */
 	unsigned int    job_no;
+
+	/* has the job completed? */
+	unsigned int	job_completed:1;
 };
 
 struct pfair_param;
@@ -270,6 +281,7 @@ struct rt_param {
 	 *          implementation).
 	 */
 	struct bheap_node*	heap_node;
+	struct bheap_node*	zs_node;
 	struct release_heap*	rel_heap;
 
 	/* Used by rt_domain to queue task in release list.
diff --git a/include/litmus/sched_plugin.h b/include/litmus/sched_plugin.h
index 0ccccd6..7596eeb 100644
--- a/include/litmus/sched_plugin.h
+++ b/include/litmus/sched_plugin.h
@@ -70,6 +70,12 @@ typedef long (*allocate_lock_t) (struct litmus_lock **lock, int type,
 /* This function causes the caller to sleep until the next release */
 typedef long (*complete_job_t) (void);
 
+typedef int (*get_global_criticality_t) (void);
+
+typedef void (*set_global_criticality_t) (int);
+
+typedef lt_t (*get_zero_slack_instant_t) (void);
+
 typedef long (*admit_task_t)(struct task_struct* tsk);
 
 typedef long (*wait_for_release_at_t)(lt_t release_time);
@@ -108,7 +114,9 @@ struct sched_plugin {
 
 	task_exit_t 		task_exit;
 	task_cleanup_t		task_cleanup;
-
+	get_global_criticality_t get_global_criticality;
+	set_global_criticality_t set_global_criticality;
+	get_zero_slack_instant_t get_zero_slack_instant;
 #ifdef CONFIG_LITMUS_LOCKING
 	/*	locking protocols	*/
 	allocate_lock_t		allocate_lock;
diff --git a/include/litmus/zeroslack_timer.h b/include/litmus/zeroslack_timer.h
new file mode 100644
index 0000000..5314dda
--- /dev/null
+++ b/include/litmus/zeroslack_timer.h
@@ -0,0 +1,8 @@
+#ifndef _LITMUS_ZERO_SLACK_H_
+#define _LITMUS_ZERO_SLACK_H_
+
+/* Update the per-processor enforcement timer (arm/reproram/cancel) for
+ * the next task. */
+void update_slack_enforcement_timer(struct task_struct* );
+
+#endif
diff --git a/litmus/Makefile b/litmus/Makefile
index 84b173a..797d966 100644
--- a/litmus/Makefile
+++ b/litmus/Makefile
@@ -6,6 +6,7 @@ obj-y     = sched_plugin.o litmus.o \
 	    preempt.o \
 	    litmus_proc.o \
 	    budget.o \
+	    zeroslack_timer.o \
 	    clustered.o \
 	    jobs.o \
 	    sync.o \
@@ -21,7 +22,8 @@ obj-y     = sched_plugin.o litmus.o \
 	    uncachedev.o \
 	    sched_gsn_edf.o \
 	    sched_psn_edf.o \
-	    sched_pfp.o
+	    sched_pfp.o \
+	    sched_zsrm.o
 
 obj-$(CONFIG_PLUGIN_CEDF) += sched_cedf.o
 obj-$(CONFIG_PLUGIN_PFAIR) += sched_pfair.o
diff --git a/litmus/budget.c b/litmus/budget.c
index 1ffb8e3..9037665 100644
--- a/litmus/budget.c
+++ b/litmus/budget.c
@@ -6,6 +6,7 @@
 #include <litmus/preempt.h>
 
 #include <litmus/budget.h>
+#include <litmus/zeroslack_timer.h>
 
 struct enforcement_timer {
 	/* The enforcement timer is used to accurately police
@@ -97,6 +98,7 @@ void update_enforcement_timer(struct task_struct* t)
 		/* Make sure we don't cause unnecessary interrupts. */
 		cancel_enforcement_timer(et);
 	}
+	update_slack_enforcement_timer(t);
 }
 
 
diff --git a/litmus/sched_zsrm.c b/litmus/sched_zsrm.c
new file mode 100644
index 0000000..fd272ed
--- /dev/null
+++ b/litmus/sched_zsrm.c
@@ -0,0 +1,664 @@
+/*
+ * litmus/sched_zsrm.c
+ *
+ * Implementation of Rate monotonic Scheduling.
+ * Based on P-FP.
+ */
+
+#include <linux/percpu.h>
+#include <linux/sched.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+
+#include <litmus/litmus.h>
+#include <litmus/wait.h>
+#include <litmus/jobs.h>
+#include <litmus/preempt.h>
+#include <litmus/fp_common.h>
+#include <litmus/binheap.h>
+#include <litmus/sched_plugin.h>
+#include <litmus/sched_trace.h>
+#include <litmus/trace.h>
+#include <litmus/budget.h>
+#include <litmus/zeroslack_timer.h>
+
+/* to set up domain/cpu mappings */
+#include <litmus/litmus_proc.h>
+#include <linux/uaccess.h>
+
+typedef struct {
+        rt_domain_t             domain;
+        struct fp_prio_queue    ready_queue;
+        int                     cpu;
+        int                     global_criticality;
+        struct bheap            rm_heap;
+        struct bheap            zs_heap;
+        struct task_struct*     scheduled; /* only RT tasks */
+/*
+ * scheduling lock slock
+ * protects the domain and serializes scheduling decisions
+ */
+#define slock domain.ready_lock
+
+} zsrm_domain_t;
+
+DEFINE_PER_CPU(zsrm_domain_t, zsrm_domains);
+
+zsrm_domain_t* zsrm_doms[NR_CPUS];
+
+#define local_zsrm               (&__get_cpu_var(zsrm_domains))
+#define remote_dom(cpu)         (&per_cpu(zsrm_domains, cpu).domain)
+#define remote_zsrm(cpu) (&per_cpu(zsrm_domains, cpu))
+#define task_dom(task)          remote_dom(get_partition(task))
+#define task_zsrm(task)          remote_zsrm(get_partition(task))
+
+
+#ifdef CONFIG_LITMUS_LOCKING
+DEFINE_PER_CPU(uint64_t,fmlp_timestamp1);
+#endif
+
+int rm_ready_order(struct bheap_node* a, struct bheap_node* b)
+{ 
+  struct task_struct *first_task;
+  struct task_struct *second_task;
+
+  first_task = bheap2task(a);
+  second_task = bheap2task(b);
+
+  if (get_rt_period(first_task) <= get_rt_period(second_task))
+    return 1;
+  else
+    return 0;
+}
+
+int zs_ready_order(struct bheap_node* a, struct bheap_node* b)
+{ 
+  struct task_struct *first_task;
+  struct task_struct *second_task;
+
+  first_task = bheap2task(a);
+  second_task = bheap2task(b);
+
+  if ((get_rt_relative_zero_slack(first_task) + get_release(first_task)) <= (get_rt_relative_zero_slack(second_task) + get_release(second_task)))
+    return 1;
+  else
+    return 0;
+}
+
+static int zsrm_get_criticality(void) 
+{
+    zsrm_domain_t*   zsrm = local_zsrm;
+    return zsrm->global_criticality;
+}
+
+static void zsrm_set_criticality(int mode) 
+{
+    zsrm_domain_t*   zsrm = local_zsrm;
+    struct task_struct* t;
+    struct bheap_node* hn;
+    
+    if(!bheap_empty(&zsrm->zs_heap))
+    {
+        hn = bheap_take(zs_ready_order, &zsrm->zs_heap);
+        t = bheap2task(hn);
+        if((is_job_running(t))){
+            TRACE_TASK(t,"Criticality mode changed to %d\n", mode);
+            zsrm->global_criticality = mode;    
+        }
+        else{
+            TRACE_TASK(t,"Job completed before zero_slack_instant\n");
+        }
+        bheap_node_free(hn);
+    }
+}
+
+static lt_t zsrm_get_zero_slack_instant(void)
+{
+    zsrm_domain_t*   zsrm = local_zsrm;
+    struct task_struct* t;
+    struct bheap_node* hn;
+
+    if(!bheap_empty(&zsrm->zs_heap))
+    {
+        hn = bheap_peek(zs_ready_order, &zsrm->zs_heap);
+        t = bheap2task(hn);
+        return (get_rt_relative_zero_slack(t) + get_release(t));
+    }
+    else
+        return 0;
+}
+
+static void update_mode_to_userspace(struct task_struct* t)
+{   
+    struct control_page* cp;
+    zsrm_domain_t*   zsrm = local_zsrm;
+
+    if(has_control_page(t)){
+        cp  = get_control_page(t);
+        cp->active_crit = zsrm->global_criticality;
+        if((zsrm->global_criticality) && (get_criticality(t) == HIGH)){
+            get_exec_cost(t) = get_exec_cost_hi(t);
+            TRACE_TASK(t,"get_exec_cost_hi = %llu\n", get_exec_cost(t));
+        }
+    }
+    else{
+        TRACE_TASK(t,"control_page set error\n");
+    }
+}
+
+/* we assume the lock is being held */
+static void preempt(zsrm_domain_t *zsrm)
+{
+        preempt_if_preemptable(zsrm->scheduled, zsrm->cpu);
+}
+
+static unsigned int priority_index(struct task_struct* t)
+{
+#ifdef CONFIG_LITMUS_LOCKING
+        if (unlikely(t->rt_param.inh_task))
+                /* use effective priority */
+                t = t->rt_param.inh_task;
+
+        if (is_priority_boosted(t)) {
+                /* zero is reserved for priority-boosted tasks */
+                return 0;
+        } else
+#endif
+                return get_priority(t);
+}
+
+static void zsrm_release_jobs(rt_domain_t* rt, struct bheap* tasks)
+{
+        zsrm_domain_t *zsrm = container_of(rt, zsrm_domain_t, domain);
+        unsigned long flags;
+        struct task_struct* t;
+        struct bheap_node* hn;
+
+        raw_spin_lock_irqsave(&zsrm->slock, flags);
+
+        while (!bheap_empty(tasks)) {
+            hn = bheap_take(fp_ready_order, tasks);
+            t = bheap2task(hn);
+            if (zsrm->global_criticality <= get_criticality(t)){
+                    TRACE_TASK(t, "released (part:%d prio:%u time_rel: %llu zero_slack: %llu\n",
+                       get_partition(t), get_priority(t), get_release(t), get_rt_relative_zero_slack(t));
+                    fp_prio_add(&zsrm->ready_queue, t, priority_index(t));
+                    tsk_rt(t)->zs_node = bheap_node_alloc(GFP_ATOMIC);
+                    if (likely(tsk_rt(t)->zs_node)) {
+                        bheap_node_init(&tsk_rt(t)->zs_node, (void*)(t));
+                        bheap_insert(zs_ready_order, &zsrm->zs_heap, tsk_rt(t)->zs_node);
+                        get_job_completed(t) = 0;
+                    }
+            }
+            else{
+                /*Active low criticality tasks has been blocked, while adding to ready or release queue*/
+            }
+        }
+
+        /* do we need to preempt? */
+        if (fp_higher_prio(fp_prio_peek(&zsrm->ready_queue), zsrm->scheduled)) {
+                TRACE_CUR("preempted by new release\n");
+                preempt(zsrm);
+        }
+
+        raw_spin_unlock_irqrestore(&zsrm->slock, flags);
+}
+
+static void zsrm_preempt_check(zsrm_domain_t *zsrm)
+{
+        if (fp_higher_prio(fp_prio_peek(&zsrm->ready_queue), zsrm->scheduled))
+                preempt(zsrm);
+}
+
+static void zsrm_domain_init(zsrm_domain_t* zsrm,
+                               int cpu)
+{
+        fp_domain_init(&zsrm->domain, NULL, zsrm_release_jobs);
+        zsrm->cpu                = cpu;
+        zsrm->scheduled          = NULL;
+        fp_prio_queue_init(&zsrm->ready_queue);
+        zsrm->global_criticality = LOW;
+        bheap_init(&zsrm->rm_heap);
+        bheap_init(&zsrm->zs_heap);
+}
+
+static void requeue(struct task_struct* t, zsrm_domain_t *zsrm)
+{
+        BUG_ON(!is_running(t));
+        tsk_rt(t)->completed = 0;
+        if (zsrm->global_criticality <= get_criticality(t)) {
+            if (is_released(t, litmus_clock())) {
+                    fp_prio_add(&zsrm->ready_queue, t, priority_index(t));
+            }
+            else{
+                    add_release(&zsrm->domain, t); /* it has got to wait */
+            }
+        }
+        else{
+            /*Active low criticality tasks has been blocked, while adding to ready or release queue*/
+        }
+}
+
+static void job_completion(struct task_struct* t, int forced)
+{
+        sched_trace_task_completion(t,forced);
+
+        tsk_rt(t)->completed = 0;
+        get_job_completed(t) = 1;
+        TRACE_TASK(t, "job_completion() %llu execution time = %llu\n", litmus_clock(), get_exec_time(t));
+        prepare_for_next_period(t);
+        if (is_released(t, litmus_clock()))
+                sched_trace_task_release(t);
+}
+
+static struct task_struct* zsrm_schedule(struct task_struct * prev)
+{
+        zsrm_domain_t*   zsrm = local_zsrm;
+        struct task_struct*     next;
+
+        int out_of_time, sleep, preempt, np, exists, blocks, resched, migrate, mode, flag;
+
+        raw_spin_lock(&zsrm->slock);
+
+        /* sanity checking
+         * differently from gedf, when a task exits (dead)
+         * zsrm->schedule may be null and prev _is_ realtime
+         */
+        BUG_ON(zsrm->scheduled && zsrm->scheduled != prev);
+        BUG_ON(zsrm->scheduled && !is_realtime(prev));
+
+        /* (0) Determine state */
+        exists      = zsrm->scheduled != NULL;
+        blocks      = exists && !is_running(zsrm->scheduled);
+        out_of_time = exists && budget_enforced(zsrm->scheduled) &&
+                                budget_exhausted(zsrm->scheduled);
+        np          = exists && is_np(zsrm->scheduled);
+        sleep       = exists && is_completed(zsrm->scheduled);
+        migrate     = exists && get_partition(zsrm->scheduled) != zsrm->cpu;
+        preempt     = !blocks && (migrate || fp_preemption_needed(&zsrm->ready_queue, prev));
+        mode        = zsrm->global_criticality;
+
+        /* If we need to preempt do so.
+         * The following checks set resched to 1 in case of special
+         * circumstances.
+         */
+        resched = preempt;
+
+        /* If a task blocks we have no choice but to reschedule.
+         */
+        if (blocks)
+                resched = 1;
+
+        /* Request a sys_exit_np() call if we would like to preempt but cannot.
+         * Multiple calls to request_exit_np() don't hurt.
+         */
+        if (np && (out_of_time || preempt || sleep))
+                request_exit_np(zsrm->scheduled);
+
+        /* Any task that is preemptable and either exhausts its execution
+         * budget or wants to sleep completes. We may have to reschedule after
+         * this.
+         */
+        
+        if (!np && (out_of_time || sleep) && !blocks && !migrate) {
+                job_completion(zsrm->scheduled, !sleep);
+                resched = 1;
+        }
+
+        if (exists && !np && mode && (mode > get_criticality(zsrm->scheduled))) {
+            TRACE_TASK(zsrm->scheduled, "LOW criticality job is moved to complete state\n");
+            job_completion(zsrm->scheduled, !sleep);
+            resched = 1;
+        }
+
+        /* The final scheduling decision. Do we need to switch for some reason?
+         * Switch if we are in RT mode and have no task or if we need to
+         * resched.
+         */
+        next = NULL;
+        if ((!np || blocks) && (resched || !exists)) {
+                /* When preempting a task that does not block, then
+                 * re-insert it into either the ready queue or the
+                 * release queue (if it completed). requeue() picks
+                 * the appropriate queue.
+                 */
+                if (zsrm->scheduled && !blocks  && !migrate)
+                        requeue(zsrm->scheduled, zsrm);
+                flag = 1;
+                while(flag){
+                    next = fp_prio_take(&zsrm->ready_queue);
+                    if(next){
+                        if(mode && get_criticality(next) == LOW){
+                            TRACE_TASK(next, "LOW critic job dropped 1\n"); 
+                        }
+                        else{
+                            update_mode_to_userspace(next);
+                            flag = 0;
+                        }
+                    }
+                    else
+                        flag = 0;
+                }
+                if (next == prev) {
+                        struct task_struct *t = fp_prio_peek(&zsrm->ready_queue);
+                        TRACE_TASK(next, "next==prev sleep=%d oot=%d np=%d preempt=%d migrate=%d "
+                                   "boost=%d empty=%d prio-idx=%u prio=%u\n",
+                                   sleep, out_of_time, np, preempt, migrate,
+                                   is_priority_boosted(next),
+                                   t == NULL,
+                                   priority_index(next),
+                                   get_priority(next));
+                        if (t)
+                                TRACE_TASK(t, "waiter boost=%d prio-idx=%u prio=%u\n",
+                                           is_priority_boosted(t),
+                                           priority_index(t),
+                                           get_priority(t));
+                }
+                /* If preempt is set, we should not see the same task again. */
+                BUG_ON(preempt && next == prev);
+                /* Similarly, if preempt is set, then next may not be NULL,
+                 * unless it's a migration. */
+                BUG_ON(preempt && !migrate && next == NULL);
+        }
+        else {
+            /* Only override Linux scheduler if we have a real-time task
+             * scheduled that needs to continue.
+             */
+            if (exists)
+                next = prev;
+        }
+
+        if (next) {
+                TRACE_TASK(next, "scheduled at %llu\n", litmus_clock());
+        } else {
+                //TRACE("becoming idle at %llu\n", litmus_clock());
+        }
+
+        zsrm->scheduled = next;
+        sched_state_task_picked();
+        raw_spin_unlock(&zsrm->slock);
+
+        return next;
+}
+
+#ifdef CONFIG_LITMUS_LOCKING
+
+/* prev is no longer scheduled --- see if it needs to migrate */
+static void zsrm_finish_switch(struct task_struct *prev)
+{
+        zsrm_domain_t *to;
+
+        if (is_realtime(prev) &&
+            is_running(prev) &&
+            get_partition(prev) != smp_processor_id()) {
+                TRACE_TASK(prev, "needs to migrate from P%d to P%d\n",
+                           smp_processor_id(), get_partition(prev));
+
+                to = task_zsrm(prev);
+
+                raw_spin_lock(&to->slock);
+
+                TRACE_TASK(prev, "adding to queue on P%d\n", to->cpu);
+                requeue(prev, to);
+                if (fp_preemption_needed(&to->ready_queue, to->scheduled))
+                        preempt(to);
+
+                raw_spin_unlock(&to->slock);
+
+        }
+}
+
+#endif
+
+/*      Prepare a task for running in RT mode
+ */
+static void zsrm_task_new(struct task_struct * t, int on_rq, int is_scheduled)
+{
+        zsrm_domain_t*   zsrm = task_zsrm(t);
+        unsigned long           flags;
+
+        TRACE_TASK(t, "P-ZSRM: task new, cpu = %d\n",
+                   t->rt_param.task_params.cpu);
+
+        /* setup job parameters */
+        release_at(t, litmus_clock());
+
+        /*Add task to RM priority assignment - binary heap*/
+        bheap_add(rm_ready_order, &zsrm->rm_heap, (void *)t, GFP_ATOMIC);
+        update_mode_to_userspace(t);
+
+        raw_spin_lock_irqsave(&zsrm->slock, flags);
+        if (is_scheduled) {
+                /* there shouldn't be anything else running at the time */
+                BUG_ON(zsrm->scheduled);
+                zsrm->scheduled = t;
+        } else if (is_running(t)) {
+                requeue(t, zsrm);
+                /* maybe we have to reschedule */
+                zsrm_preempt_check(zsrm);
+        }
+        raw_spin_unlock_irqrestore(&zsrm->slock, flags);
+}
+
+static void zsrm_task_wake_up(struct task_struct *task)
+{
+        unsigned long           flags;
+        zsrm_domain_t*           zsrm = task_zsrm(task);
+        lt_t                    now;
+        struct task_struct *t;
+        struct bheap_node* rm_hn;
+        unsigned long long prio = 1;
+
+        while (!bheap_empty(&zsrm->rm_heap)) {
+                rm_hn = bheap_take(rm_ready_order, &zsrm->rm_heap);
+                t = bheap2task(rm_hn);
+                get_priority(t) = prio;
+                prio++;
+                bheap_node_free(rm_hn);
+                TRACE_TASK(t, "changing prio:%d\n", get_priority(t));
+        }
+
+        TRACE_TASK(task, "wake_up at %llu\n", litmus_clock());
+        raw_spin_lock_irqsave(&zsrm->slock, flags);
+
+#ifdef CONFIG_LITMUS_LOCKING
+        /* Should only be queued when processing a fake-wake up due to a
+         * migration-related state change. */
+        if (unlikely(is_queued(task))) {
+                TRACE_TASK(task, "WARNING: waking task still queued. Is this right?\n");
+                goto out_unlock;
+        }
+#else
+        BUG_ON(is_queued(task));
+#endif
+        now = litmus_clock();
+        if (is_sporadic(task) && is_tardy(task, now)
+#ifdef CONFIG_LITMUS_LOCKING
+        /* We need to take suspensions because of semaphores into
+         * account! If a job resumes after being suspended due to acquiring
+         * a semaphore, it should never be treated as a new job release.
+         */
+            && !is_priority_boosted(task)
+#endif
+                ) {
+                /* new sporadic release */
+                release_at(task, now);
+                sched_trace_task_release(task);
+        }
+
+        /* Only add to ready queue if it is not the currently-scheduled
+         * task. This could be the case if a task was woken up concurrently
+         * on a remote CPU before the executing CPU got around to actually
+         * de-scheduling the task, i.e., wake_up() raced with schedule()
+         * and won. Also, don't requeue if it is still queued, which can
+         * happen under the DPCP due wake-ups racing with migrations.
+         */
+        if (zsrm->scheduled != task) {
+                requeue(task, zsrm);
+                zsrm_preempt_check(zsrm);
+        }
+
+#ifdef CONFIG_LITMUS_LOCKING
+out_unlock:
+#endif
+        raw_spin_unlock_irqrestore(&zsrm->slock, flags);
+        TRACE_TASK(task, "wake up done\n");
+}
+
+static void zsrm_task_block(struct task_struct *t)
+{
+        /* only running tasks can block, thus t is in no queue */
+        TRACE_TASK(t, "block at %llu, state=%d\n", litmus_clock(), t->state);
+
+        BUG_ON(!is_realtime(t));
+
+        /* If this task blocked normally, it shouldn't be queued. The exception is
+         * if this is a simulated block()/wakeup() pair from the pull-migration code path.
+         * This should only happen if the DPCP is being used.
+         */
+#ifdef CONFIG_LITMUS_LOCKING
+        if (unlikely(is_queued(t)))
+                TRACE_TASK(t, "WARNING: blocking task still queued. Is this right?\n");
+#else
+        BUG_ON(is_queued(t));
+#endif
+}
+
+static void zsrm_task_exit(struct task_struct * t)
+{
+        unsigned long flags;
+        zsrm_domain_t*   zsrm = task_zsrm(t);
+        rt_domain_t*            dom;
+
+        raw_spin_lock_irqsave(&zsrm->slock, flags);
+        if (is_queued(t)) {
+                BUG(); /* This currently doesn't work. */
+                /* dequeue */
+                dom  = task_dom(t);
+                remove(dom, t);
+        }
+        if (zsrm->scheduled == t) {
+                zsrm->scheduled = NULL;
+                preempt(zsrm);
+        }
+        TRACE_TASK(t, "RIP, now reschedule\n");
+
+        raw_spin_unlock_irqrestore(&zsrm->slock, flags);
+}
+
+static long zsrm_admit_task(struct task_struct* tsk)
+{
+        if (task_cpu(tsk) == tsk->rt_param.task_params.cpu &&
+#ifdef CONFIG_RELEASE_MASTER
+            /* don't allow tasks on release master CPU */
+            task_cpu(tsk) != remote_dom(task_cpu(tsk))->release_master &&
+#endif
+            litmus_is_valid_fixed_prio(get_priority(tsk)))
+                return 0;
+        else
+                return -EINVAL;
+}
+
+static struct domain_proc_info zsrm_domain_proc_info;
+static long zsrm_get_domain_proc_info(struct domain_proc_info **ret)
+{
+        *ret = &zsrm_domain_proc_info;
+        return 0;
+}
+
+static void zsrm_setup_domain_proc(void)
+{
+        int i, cpu;
+        int release_master =
+#ifdef CONFIG_RELEASE_MASTER
+                atomic_read(&release_master_cpu);
+#else
+                NO_CPU;
+#endif
+        int num_rt_cpus = num_online_cpus() - (release_master != NO_CPU);
+        struct cd_mapping *cpu_map, *domain_map;
+
+        memset(&zsrm_domain_proc_info, sizeof(zsrm_domain_proc_info), 0);
+        init_domain_proc_info(&zsrm_domain_proc_info, num_rt_cpus, num_rt_cpus);
+        zsrm_domain_proc_info.num_cpus = num_rt_cpus;
+        zsrm_domain_proc_info.num_domains = num_rt_cpus;
+        for (cpu = 0, i = 0; cpu < num_online_cpus(); ++cpu) {
+                if (cpu == release_master)
+                        continue;
+                cpu_map = &zsrm_domain_proc_info.cpu_to_domains[i];
+                domain_map = &zsrm_domain_proc_info.domain_to_cpus[i];
+
+                cpu_map->id = cpu;
+                domain_map->id = i; /* enumerate w/o counting the release master */
+                cpumask_set_cpu(i, cpu_map->mask);
+                cpumask_set_cpu(cpu, domain_map->mask);
+                ++i;
+        }
+}
+
+static long zsrm_activate_plugin(void)
+{
+#if defined(CONFIG_RELEASE_MASTER) || defined(CONFIG_LITMUS_LOCKING)
+        int cpu;
+#endif
+
+#ifdef CONFIG_RELEASE_MASTER
+        for_each_online_cpu(cpu) {
+                remote_dom(cpu)->release_master = atomic_read(&release_master_cpu);
+        }
+#endif
+
+#ifdef CONFIG_LITMUS_LOCKING
+        for_each_online_cpu(cpu) {
+                zsrm_doms[cpu] = remote_zsrm(cpu);
+        }
+#endif
+
+        zsrm_setup_domain_proc();
+
+        return 0;
+}
+
+static long zsrm_deactivate_plugin(void)
+{
+        destroy_domain_proc_info(&zsrm_domain_proc_info);
+        return 0;
+}
+
+/*      Plugin object   */
+static struct sched_plugin zsrm_plugin __cacheline_aligned_in_smp = {
+        .plugin_name            = "P-ZSRM",
+        .task_new               = zsrm_task_new,
+        .complete_job           = complete_job,
+        .task_exit              = zsrm_task_exit,
+        .schedule               = zsrm_schedule,
+        .task_wake_up           = zsrm_task_wake_up,
+        .task_block             = zsrm_task_block,
+        .admit_task             = zsrm_admit_task,
+        .activate_plugin        = zsrm_activate_plugin,
+        .deactivate_plugin      = zsrm_deactivate_plugin,
+        .get_domain_proc_info   = zsrm_get_domain_proc_info,
+#ifdef CONFIG_LITMUS_LOCKING
+        .finish_switch          = zsrm_finish_switch,
+        .get_global_criticality = zsrm_get_criticality,
+        .set_global_criticality = zsrm_set_criticality,
+        .get_zero_slack_instant = zsrm_get_zero_slack_instant,
+#endif
+};
+
+static int __init init_zsrm(void)
+{
+        int i;
+
+        /* We do not really want to support cpu hotplug, do we? ;)
+         * However, if we are so crazy to do so,
+         * we cannot use num_online_cpu()
+         */
+        for (i = 0; i < num_online_cpus(); i++) {
+                zsrm_domain_init(remote_zsrm(i), i);
+        }
+        return register_sched_plugin(&zsrm_plugin);
+}
+
+module_init(init_zsrm);
diff --git a/litmus/zeroslack_timer.c b/litmus/zeroslack_timer.c
new file mode 100644
index 0000000..b6934b8
--- /dev/null
+++ b/litmus/zeroslack_timer.c
@@ -0,0 +1,110 @@
+#include <linux/sched.h>
+#include <linux/percpu.h>
+#include <linux/hrtimer.h>
+
+#include <litmus/litmus.h>
+#include <litmus/preempt.h>
+#include <litmus/sched_plugin.h>
+#include <litmus/zeroslack_timer.h>
+
+struct slack_enforcement_timer {
+	/* The slack_enforcement_timer is used to monitor zero slack instant */
+	struct hrtimer		timer;
+	int			armed;
+};
+
+DEFINE_PER_CPU(struct slack_enforcement_timer, slack_timer);
+
+static enum hrtimer_restart on_zero_slack_instant(struct hrtimer *timer)
+{
+	struct slack_enforcement_timer* et = container_of(timer,
+						    struct slack_enforcement_timer,
+						    timer);
+	unsigned long flags;
+
+	local_irq_save(flags);
+	et->armed = 0;
+
+	TRACE("on_zero_slack_instant at %llu\n", litmus_clock());
+	/* activate scheduler */
+	litmus->set_global_criticality(HIGH);
+	if (litmus->get_global_criticality() == HIGH)
+		litmus_reschedule_local();
+	local_irq_restore(flags);
+
+	return  HRTIMER_NORESTART;
+}
+
+/* assumes called with IRQs off */
+static void cancel_slack_enforcement_timer(struct slack_enforcement_timer* et)
+{
+	int ret;
+
+	TRACE("cancelling slack instant enforcement timer at %llu\n", litmus_clock());
+
+	/* Since interrupts are disabled and et->armed is only
+	 * modified locally, we do not need any locks.
+	 */
+
+	if (et->armed) {
+		ret = hrtimer_try_to_cancel(&et->timer);
+		/* Should never be inactive. */
+		BUG_ON(ret == 0);
+		/* Should never be running concurrently. */
+		BUG_ON(ret == -1);
+
+		et->armed = 0;
+	}
+}
+
+/* assumes called with IRQs off */
+static void arm_slack_enforcement_timer(struct slack_enforcement_timer* et,
+				  struct task_struct* t)
+{
+	lt_t when_to_fire;
+
+	WARN_ONCE(!hrtimer_is_hres_active(&et->timer),
+		KERN_ERR "WARNING: no high resolution timers available!?\n");
+	/* __hrtimer_start_range_ns() cancels the timer
+	 * anyway, so we don't have to check whether it is still armed */
+
+	if (likely(!is_np(t)) && (litmus->get_zero_slack_instant())) {
+		when_to_fire = litmus->get_zero_slack_instant();
+		TRACE_TASK(t, "when_to_fire at arm_slack_enforcement_timer %llu\n", when_to_fire);
+		__hrtimer_start_range_ns(&et->timer,
+					 ns_to_ktime(when_to_fire),
+					 0 /* delta */,
+					 HRTIMER_MODE_ABS_PINNED,
+					 0 /* no wakeup */);
+		et->armed = 1;
+		TRACE_TASK(t, "arming slack instant enforcement timer at %llu\n", litmus_clock());
+	}
+}
+
+/* expects to be called with IRQs off */
+void update_slack_enforcement_timer(struct task_struct* t)
+{
+	struct slack_enforcement_timer* et = &__get_cpu_var(slack_timer);
+
+	if (t && (litmus->get_global_criticality() == LOW) && !(et->armed)) {
+		/* Make sure we call into the scheduler when this slack
+		 * expires. */
+		arm_slack_enforcement_timer(et, t);
+	}
+}
+
+
+static int __init init_slack_enforcement(void)
+{
+	int cpu;
+	struct slack_enforcement_timer* et;
+
+	for (cpu = 0; cpu < NR_CPUS; cpu++)  {
+		et = &per_cpu(slack_timer, cpu);
+		hrtimer_init(&et->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+		et->timer.function = on_zero_slack_instant;
+	}
+	return 0;
+}
+
+module_init(init_slack_enforcement);
-- 
1.9.1

